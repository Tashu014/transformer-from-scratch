## Transformers from Scratch

This project implements the **Transformer** architecture from scratch using **Numpy** to better understand its key components: **Self-Attention**, **Multi-Head Attention**, and **Feed-Forward Networks**. The code provides a simple implementation that mimics the core functionalities of the Transformer, with a focus on the mathematical foundations behind these techniques.

- **Self-Attention**: Implements the self-attention mechanism to help the model focus on different parts of the input sequence.
  
- **Multi-Head Attention**: Extends the self-attention mechanism by allowing the model to capture different aspects of the sequence in parallel.
  
- **Transformer Block**: Combines multi-head attention and feed-forward layers to form the building blocks of a Transformer.

- **Layer Normalization**: Includes a basic implementation of layer normalization to stabilize and accelerate training.
